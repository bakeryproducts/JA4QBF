{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-13T11:15:56.430726Z",
     "start_time": "2018-12-13T11:15:54.540069Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from tensorflow.keras.losses import kld\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from CustomModels import utils \n",
    "from CustomModels import MyModel_list\n",
    "import CustomModels.metrics as metrics\n",
    "import CustomModels.rank_metrics as rank_metrics\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "# from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import Dense,Input,BatchNormalization,Dropout#,CuDNNLSTM,CuDNNGRU\n",
    "# from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Neural network model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Setting up necessary functions and sanity-checking dataset, actual thing starts at chapter 1. <br>\n",
    " By Sokolov Gleb <br>\n",
    " Part 2 of job application @ ООО ИК QBF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-12T09:42:14.895881Z",
     "start_time": "2018-12-12T09:42:14.887451Z"
    }
   },
   "source": [
    "## 2.0 Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-13T11:56:04.295519Z",
     "start_time": "2018-12-13T11:56:04.287158Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1.12.0', '2.1.6-tf')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__,tf.keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-13T11:15:56.438120Z",
     "start_time": "2018-12-13T11:15:56.433180Z"
    }
   },
   "outputs": [],
   "source": [
    "# wrapper for tf - to - keras metrics usage\n",
    "def as_keras_metric(method):\n",
    "    import functools\n",
    "    @functools.wraps(method)\n",
    "    def wrapper(self, args, **kwargs):\n",
    "        \"\"\" Wrapper for turning tensorflow metrics into keras metrics \"\"\"\n",
    "        value, update_op = method(self, args, **kwargs)\n",
    "        K.get_session().run(tf.local_variables_initializer())\n",
    "        with tf.control_dependencies([update_op]):\n",
    "            value = tf.identity(value)\n",
    "        return value\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-13T11:15:56.463449Z",
     "start_time": "2018-12-13T11:15:56.440265Z"
    }
   },
   "outputs": [],
   "source": [
    "# experimenting with metrics, custom tf metrics is a no go at the moment for Keras \n",
    "ndcg_metric = as_keras_metric(metrics.make_ranking_metric_fn(metrics.RankingMetricKey().DCG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-13T11:15:56.484534Z",
     "start_time": "2018-12-13T11:15:56.466175Z"
    }
   },
   "outputs": [],
   "source": [
    "# Testing set at given linear regression problem,things are getting explained in part 1.\n",
    "m = 1000\n",
    "n = 5\n",
    "f = 1\n",
    "qs = ['date_'+str(i) for i in range(m)]\n",
    "qs = np.array(sorted(qs*n))\n",
    "scores = np.random.random(m*n)\n",
    "features = (2*scores+np.random.normal(0,.1,m*n)).reshape((f,-1))\n",
    "#features = np.random.random(m*n*f).reshape((f,-1))\n",
    "features = [fi.astype('float32') for fi in features]\n",
    "feature_names = ['F'+str(i+1) for i in range(f)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-13T11:15:56.513535Z",
     "start_time": "2018-12-13T11:15:56.489021Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q_id</th>\n",
       "      <th>Score</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>date_0</td>\n",
       "      <td>0.985924</td>\n",
       "      <td>1.882903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>date_0</td>\n",
       "      <td>0.154298</td>\n",
       "      <td>0.313252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>date_0</td>\n",
       "      <td>0.799794</td>\n",
       "      <td>1.713730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>date_0</td>\n",
       "      <td>0.335992</td>\n",
       "      <td>0.637290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>date_0</td>\n",
       "      <td>0.175667</td>\n",
       "      <td>0.494688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>date_1</td>\n",
       "      <td>0.667605</td>\n",
       "      <td>1.312209</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Q_id     Score        F1\n",
       "0  date_0  0.985924  1.882903\n",
       "1  date_0  0.154298  0.313252\n",
       "2  date_0  0.799794  1.713730\n",
       "3  date_0  0.335992  0.637290\n",
       "4  date_0  0.175667  0.494688\n",
       "5  date_1  0.667605  1.312209"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sample dataframe data example \n",
    "df = pd.DataFrame(dict(zip(['Q_id','Score',*feature_names],[qs,scores,*features])))\n",
    "df.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-13T11:15:57.805166Z",
     "start_time": "2018-12-13T11:15:56.516659Z"
    }
   },
   "outputs": [],
   "source": [
    "# either this or next block should be run\n",
    "#groupng stocks at each trading day (Queue)\n",
    "d = df.groupby('Q_id').groups.values()\n",
    "X = np.array([df.loc[g,feature_names].values.T[0] for g in d])\n",
    "y = np.array([df.loc[g,'Score'].values for g in d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-13T11:15:57.813355Z",
     "start_time": "2018-12-13T11:15:57.808688Z"
    }
   },
   "outputs": [],
   "source": [
    "# Do simple all in one multi-dimensional space\n",
    "X = df[feature_names].values\n",
    "y = df.Score.values.reshape((-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Adaptation of ListNet for ranking stocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, there is no Keras version of Listnet that I found, so lets get educated and implement maths. <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To adapt our problem to ranking scheme, lets get along with great paper for listwise ranking learning: [\"Learning to Rank: From Pairwise Approach to Listwise Approach\"](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-2007-40.pdf).<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In training, we have a list of stocks for each trading day: $ D = \\{d^{(1)},d^{(2)}, ... , d^{(n)} \\}$. Each day associated with a list of stocks $ S^{(i)} = \\{s^{(i)}_1, s^{(i)}_2, ... , s^{(i)}_m \\}$. Furthermore, each stock $S^{(i)}$ is associated with list of independent ranking scores $ y^{(i)} = \\{y^{(i)}_1, y^{(i)}_2, ... , y^{(i)}_m \\}$ <br>\n",
    "\n",
    "For now i will go only with $S^{(i)}$ data, droping possible impact of $D$. In the other way it will recuire redoing tensorflow computation graph from scratch, and this research is more of display of skill kinda work.  So $x^{(i)}_j = \\Psi (s^{(i)}_j)$, $i = 1,2,\\dots,n, j = 1,2,\\dots,m$. <br>\n",
    "\n",
    "Each list of features $x^{(i)}= (x^{(i)}_1, x^{(i)}_2, ... , x^{(i)}_m)$ and the corresponding list of given ranks $ y^{(i)} =  (y^{(i)}_1, y^{(i)}_2, ... , y^{(i)}_m )$ then form an \"instance\". Training set can be denoted as $ \\mathcal{T} = \\big\\{ (x^{(i)},y^{(i)}) \\big\\}^m_{i=1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create a ranking function $f$ for each feature vector $x^{(i)}_j$ (corresponding to stock $s^{(i)}_j$) it outputs a score $f(x^{(i)}_j)$. For list of feature vectors we obtain a list of scores $z^{(i)} = \\big(f(x^{(i)}_1),f(x^{(i)}_2),\\dots,f(x^{(i)}_m) \\big)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of learning is formalized as minimization of the total losses with respect to the training data.\n",
    "$$ \\sum_{i=1}^{m} L(y^{(i)},z^{(i)})$$\n",
    "where $L$ is a listwise loss function. I choose to use Jensen-Shannon divergence as loss function:\n",
    "$$ D_{js}(P,Q) = \\frac{1}{2} D_{kl}(P,M) + \\frac{1}{2} D_{kl}(Q,M)$$\n",
    "$$M = \\frac{1}{2}(P+Q) $$\n",
    "$$D_{kl}(P,Q) = - \\sum_{i} P(i) \\log \\bigg(\\frac{Q(i)}{P(i)} \\bigg) $$\n",
    "where P and Q are dicrete probability distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Model wrapped in its own class, object-oriented-style ( @ `./CustomModels/MyModel_list.py`). Class had some flexible options on how to construct NN model, such as : \n",
    "\n",
    "1. Test/Validation ratio\n",
    "2. Usage of regularization (Elastic, l1 and l2)\n",
    "3. Usage of Dropout layer\n",
    "4. Number of layers\n",
    "5. Number of neurons (in first layer, rest will decay as n//2)\n",
    "6. Epochs, window size, batch size\n",
    "\n",
    "All test are logged in Tensorboard format, stored @ `./data/logs/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At last, lets check our hardware, for running tf on GPU we need NVIDIA-CUDA graphics card, all drivers preinstalled. Im running Linux Manjaro with bumblebee switch, notebook should be started as <br>`[user]$ optirun jupyter-notebook`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 NN graph (via Tensorboard)\n",
    "<img src = 'graph2.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-13T11:57:37.859835Z",
     "start_time": "2018-12-13T11:57:37.848500Z"
    }
   },
   "outputs": [],
   "source": [
    "TEST_SPLIT,VAL_SPLIT = .1, .1\n",
    "N_NEURONS = 512\n",
    "LAYERS = 2\n",
    "FEATURES = 3\n",
    "BATCH = 128\n",
    "EPOCHS = 5\n",
    "DROPOUT = .2\n",
    "TICKER = 'TECH_SECTOR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-13T11:57:38.919062Z",
     "start_time": "2018-12-13T11:57:38.644085Z"
    }
   },
   "outputs": [],
   "source": [
    "# market ranks data\n",
    "with open('./data/ranks.pkl','rb') as f:\n",
    "    market = pickle.load(f)\n",
    "# on my hardware it runs kinda slow, so i will stick with just one sector\n",
    "# if needed, model can be wraped into function, like it is in ./RNN_1.5*\n",
    "market = market.Technology.dropna()\n",
    "market = market.reindex(sorted(market.columns),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-13T11:57:38.924563Z",
     "start_time": "2018-12-13T11:57:38.921271Z"
    }
   },
   "outputs": [],
   "source": [
    "market = market.iloc[:,:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-13T11:57:39.187496Z",
     "start_time": "2018-12-13T11:57:39.175873Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = market.loc[:,(slice(None),('P/E_rank','PEG_rank','mom_rank'))].values\n",
    "y = market.loc[:,(slice(None),('r_sum_rank'))].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-13T11:57:40.902961Z",
     "start_time": "2018-12-13T11:57:40.889600Z"
    }
   },
   "outputs": [],
   "source": [
    "stock_model = MyModel_list.StockPred(X,y,\n",
    "                                    test_ratio=TEST_SPLIT,\n",
    "                                    val_ratio=VAL_SPLIT,\n",
    "                        ticker=TICKER,\n",
    "                        use_reg = True,\n",
    "                        neurons=N_NEURONS,\n",
    "                        layers=LAYERS,\n",
    "                        features=FEATURES,\n",
    "                        batch_size=BATCH,\n",
    "                        epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-13T12:02:28.314590Z",
     "start_time": "2018-12-13T11:57:42.347962Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (InputLayer)           (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "Dense_512 (Dense)            (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "Dropout_0.2_0 (Dropout)      (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Normalization_0 (BatchNormal (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "Dense_256 (Dense)            (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "Dropout_0.2_1 (Dropout)      (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Normalization_1 (BatchNormal (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 136,705\n",
      "Trainable params: 135,169\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n",
      "Train on 318060 samples, validate on 35340 samples\n",
      "Epoch 1/5\n",
      "318060/318060 [==============================] - 57s 178us/step - loss: 0.7836 - mean_squared_error: 0.8406 - val_loss: 0.6446 - val_mean_squared_error: 0.8696\n",
      "Epoch 2/5\n",
      "318060/318060 [==============================] - 57s 179us/step - loss: 0.6374 - mean_squared_error: 0.7749 - val_loss: 1.1783 - val_mean_squared_error: 1.3296\n",
      "Epoch 3/5\n",
      "318060/318060 [==============================] - 57s 179us/step - loss: 0.7685 - mean_squared_error: 0.9900 - val_loss: 0.6223 - val_mean_squared_error: 0.9502\n",
      "Epoch 4/5\n",
      "318060/318060 [==============================] - 57s 179us/step - loss: 0.6363 - mean_squared_error: 1.1014 - val_loss: 0.8039 - val_mean_squared_error: 1.2623\n",
      "Epoch 5/5\n",
      "318060/318060 [==============================] - 57s 179us/step - loss: 0.6593 - mean_squared_error: 1.0842 - val_loss: 0.7918 - val_mean_squared_error: 1.2046\n",
      "Model 13.12-14:57_TECH_SECTOR-5E2L3F128B512N compiled! Calculating accuracy...\n"
     ]
    }
   ],
   "source": [
    "stock_model.gen_model(use_dropout=DROPOUT,use_norm=True)\n",
    "stock_model.model.summary()\n",
    "acc = stock_model.compile(verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-13T12:04:21.541457Z",
     "start_time": "2018-12-13T12:04:20.807785Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = stock_model.test_model().ravel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-13T16:28:11.708831Z",
     "start_time": "2018-12-13T16:28:11.694956Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-0a6bd9dfc694>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstock_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(stock_model.y_test[1::20])\n",
    "plt.plot(y_pred[1::20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Links\n",
    "1. Original paper: https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-2007-40.pdf\n",
    "\n",
    "2. JSD: https://en.wikipedia.org/wiki/Jensen–Shannon_divergence\n",
    "3. KLD: https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence\n",
    "4. ELU over ReLU https://arxiv.org/abs/1511.07289\n",
    "5. Batch Normalization https://arxiv.org/abs/1502.03167"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-13T16:28:36.056621Z",
     "start_time": "2018-12-13T16:28:36.049299Z"
    }
   },
   "source": [
    "## 2.3 Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-13T11:36:18.141218Z",
     "start_time": "2018-12-13T11:15:54.596Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ls = []\n",
    "for i in np.arange(0,100,2):\n",
    "    t = 100\n",
    "    a = np.random.random(t)#np.arange(t)/10\n",
    "    b = a[:]#np.arange(t)/10\n",
    "\n",
    "    k=i\n",
    "    b = np.append(b[:k],np.random.random(t-k)/10)\n",
    "\n",
    "    a = tf.constant(a)\n",
    "    b = tf.constant(b)\n",
    "\n",
    "    t = jsd(a,b)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        ls.append(sess.run(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-13T11:36:18.142785Z",
     "start_time": "2018-12-13T11:15:54.599Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
