{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-12T07:55:46.910927Z",
     "start_time": "2018-12-12T07:55:46.030159Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_profiling \n",
    "import matplotlib.pyplot as plt\n",
    "import pickle \n",
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "from pandas_datareader import data as pdr\n",
    "import fix_yahoo_finance as yf\n",
    "yf.pdr_override()\n",
    "\n",
    "%matplotlib inline\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <br>\n",
    " By Sokolov Gleb <br>\n",
    " Part of job application @ ООО ИК QBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-12T07:55:46.945708Z",
     "start_time": "2018-12-12T07:55:46.913046Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def adjsplit(price,scale,date):\n",
    "    date = date +' 09:45:00'\n",
    "    act_price = price[date:]\n",
    "    corr_price = price[:date].iloc[:-1]/scale\n",
    "    t = pd.concat([corr_price,act_price])\n",
    "    return t\n",
    "\n",
    "def setorder(tdf,col,ascending):\n",
    "    dfs=[]\n",
    "    #ttdf = tdf.loc[:,(slice(None),slice(None),col)].rank(axis=1,pct=True)\n",
    "    for sector in sector_dict:\n",
    "        rank_df= tdf.loc[:,(sector,slice(None),col)].rank(axis=1,pct=True,ascending=ascending)\n",
    "        rank_df.columns = rank_df.columns.remove_unused_levels().set_levels(['rank'],level=2)\n",
    "\n",
    "        r_rank_df = rank_df.rolling(26*2).mean()\n",
    "        r_rank_df.columns = r_rank_df.columns.set_levels(['rank_r'],level=2)\n",
    "\n",
    "        dfs.append(rank_df)\n",
    "        #dfs.append(r_rank_df)\n",
    "    \n",
    "    ttdf = pd.concat(dfs,axis=1)\n",
    "    return ttdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Reading stocks data\n",
    "\n",
    "Reading stock tickers at sectors from `./data/sectors.csv`, combining them into dictionary.\n",
    "Dataframe with stock price and volume values is already preprocessed and loading from `./data/market_data_pd.pkl` file. Splits in stocks price and volume are settled with information from `./data/div_data.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-12T07:55:46.993526Z",
     "start_time": "2018-12-12T07:55:46.947454Z"
    }
   },
   "outputs": [],
   "source": [
    "data_sectors = pd.read_csv('./data/sectors.csv').dropna()\n",
    "sectors = data_sectors.INDUSTRY_SECTOR.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-12T07:55:47.010872Z",
     "start_time": "2018-12-12T07:55:46.998282Z"
    }
   },
   "outputs": [],
   "source": [
    "sector_dict = {sector : data_sectors[data_sectors.INDUSTRY_SECTOR == sector].TICKER.values for sector in sectors.index}\n",
    "ticker_dict = {ticker: sector for ticker,sector in zip(data_sectors.TICKER,data_sectors.INDUSTRY_SECTOR)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-12T07:55:47.025318Z",
     "start_time": "2018-12-12T07:55:47.013170Z"
    },
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# with open('./data/splits.pkl','rb') as f:\n",
    "#     data_splits = pickle.load(f)\n",
    "# columns_select = ['close','volume']\n",
    "# ttdf = []\n",
    "# for ticker,sector in ticker_dict.items():\n",
    "#     tdf = pd.read_csv(f'./data/market_data/{ticker}.csv',index_col='dt')\n",
    "#     tdf = tdf[columns_select].astype('float32')\n",
    "#     tdf.columns = pd.MultiIndex.from_product([[sector],[ticker],tdf.columns])\n",
    "#     try:\n",
    "#         splits = data_splits[ticker]\n",
    "#         print(sector,ticker,splits)\n",
    "#         for split in splits:\n",
    "#             tdf[(sector,ticker,'close')] = adjsplit(tdf[(sector,ticker,'close')],*split)        \n",
    "#             tdf[(sector,ticker,'volume')] = adjsplit(tdf[(sector,ticker,'volume')],1/split[0],split[1])        \n",
    "\n",
    "#     except Exception as e:\n",
    "#         pass\n",
    "#         #print('Error ',e)\n",
    "#     ttdf.append(tdf)\n",
    "        \n",
    "# data = pd.concat(ttdf,axis=1,sort=True)\n",
    "# data.index = pd.to_datetime(data.index)\n",
    "# data = data.reindex(sorted(data.columns), axis=1)\n",
    "# data.head()\n",
    "# with open('./data/market_data_pd.pkl','wb') as f:\n",
    "#     pickle.dump(data,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-12T07:55:47.904551Z",
     "start_time": "2018-12-12T07:55:47.027866Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 52962 entries, 2010-01-04 09:45:00 to 2018-02-05 16:00:00\n",
      "Columns: 1318 entries, (Basic Materials, AA, close) to (Utilities, XEL, volume)\n",
      "dtypes: float32(1318)\n",
      "memory usage: 266.7 MB\n"
     ]
    }
   ],
   "source": [
    "with open('./data/market_data_pd.pkl','rb') as f:\n",
    "    data_all = pickle.load(f)\n",
    "data_all.info() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-12T07:55:47.919584Z",
     "start_time": "2018-12-12T07:55:47.906283Z"
    }
   },
   "outputs": [],
   "source": [
    "# making sure on time range\n",
    "start_day,end_day = '2010-01-01','2017-01-01'\n",
    "\n",
    "data_all.index = (pd.to_datetime(data_all.index))\n",
    "data_all = data_all.loc[start_day:end_day]\n",
    "data_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-12T07:55:47.927008Z",
     "start_time": "2018-12-12T07:55:47.921334Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45812, 1318)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_all.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Filtering data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional one can do data filtering. It goes under two criteria:\n",
    "1. Volume\n",
    "2. Price\n",
    "\n",
    "Volume criteria depends on certain quantile value for distribution of mean values for each stock, grouped by their sectors:\n",
    "$$V_{sector}^{stock} = \\begin{cases}\n",
    "                              V_{sector}^{stock} & \\text{, }E (V_{sector}^{stock})\\ge [V_{Qsector}]\\\\    \n",
    "                              \\text{NaN}    \n",
    "                       \\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "For pricing it is percentile drop between two trading days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-12T07:55:47.944817Z",
     "start_time": "2018-12-12T07:55:47.931873Z"
    }
   },
   "outputs": [],
   "source": [
    "# volume_q_limit,price_pct_limit = .1,.5\n",
    "\n",
    "# data_vol = data_all.loc[:,(slice(None),slice(None),'volume')]\n",
    "# volume_limit = data_vol.mean().groupby(level=0).quantile(volume_q_limit)\n",
    "# vl_rule = [((data_vol[s].mean() >= volume_limit[s]).values) for s in sectors.keys()]\n",
    "# volume_rule = [i for subl in vl_rule for i in subl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-12T07:55:47.958279Z",
     "start_time": "2018-12-12T07:55:47.948035Z"
    }
   },
   "outputs": [],
   "source": [
    "# data_pri = data_all_range.loc[:,(slice(None),slice(None),'close')]\n",
    "# price_rule = ((np.abs(data_pri.pct_change()[1:]) >= price_pct_limit).sum() == 0)#.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-12T07:55:47.973770Z",
     "start_time": "2018-12-12T07:55:47.960439Z"
    }
   },
   "outputs": [],
   "source": [
    "# ticks = price_rule.index.get_level_values(1)\n",
    "# rule = ticks[(volume_rule & price_rule)].values\n",
    "# tdf = data_all.loc[:,(slice(None),rule)]\n",
    "# data_filtered = tdf.loc[:,(tdf.isnull().sum()<200).values].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-12T07:55:48.816865Z",
     "start_time": "2018-12-12T07:55:47.977171Z"
    }
   },
   "outputs": [],
   "source": [
    "# letting in only data with enough actual data in it\n",
    "data_all = data_all.loc[:,(data_all.isnull().sum()<200).values]\n",
    "data_all = data_all.ffill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-12T07:55:48.820834Z",
     "start_time": "2018-12-12T07:55:48.818547Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "data_filtered = data_all\n",
    "del data_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-12T07:55:48.846428Z",
     "start_time": "2018-12-12T07:55:48.823183Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45812, 1040)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_filtered.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Limiting stocks per sector\n",
    "\n",
    "For stability of future NN model I will do sampling on the stock data per sector. So I randomly choose under N stocks in each sector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-12T07:55:50.360068Z",
     "start_time": "2018-12-12T07:55:48.848186Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Sector</th>\n",
       "      <th>Consumer, Non-cyclical</th>\n",
       "      <th>Financial</th>\n",
       "      <th>Consumer, Cyclical</th>\n",
       "      <th>Industrial</th>\n",
       "      <th>Technology</th>\n",
       "      <th>Energy</th>\n",
       "      <th>Communications</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>Basic Materials</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>#</th>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Sector  Consumer, Non-cyclical  Financial  Consumer, Cyclical  Industrial  \\\n",
       "#                           20         20                  20          20   \n",
       "\n",
       "Sector  Technology  Energy  Communications  Utilities  Basic Materials  \n",
       "#               20      20              20         20               20  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_tickers_by_sector = 20\n",
    "chosen = []\n",
    "\n",
    "for s in data_filtered.columns.levels[0]:\n",
    "    temp_tickers = np.random.choice(data_filtered.loc[:,s].stack().columns,n_tickers_by_sector,replace=False)\n",
    "    temp_tickers = np.unique(temp_tickers)\n",
    "    chosen.append(data_filtered.loc[:,(slice(None),temp_tickers)])\n",
    "\n",
    "data_n_by_sector = pd.concat(chosen,axis=1)\n",
    "\n",
    "#data_n_by_sector = data_filtered\n",
    "\n",
    "#those two started to get their Earnings data only at 2014 so...\n",
    "banned_list=['CCE','MNST']\n",
    "n_tickers = data_n_by_sector.columns.get_level_values(1).drop_duplicates()\n",
    "try:\n",
    "    for banned in banned_list:\n",
    "        if banned in n_tickers:\n",
    "            n_tickers = n_tickers.drop(banned)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "sect_lens = np.array([len(data_n_by_sector.loc[:,s].stack().columns) for s in data_n_by_sector.columns.levels[0] ])\n",
    "del data_filtered\n",
    "pd.DataFrame(sect_lens,index = pd.Index(list(sector_dict.keys()),name='Sector'),columns=['#']).T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Advanced features\n",
    "\n",
    "Little bit of feature engineering;\n",
    "adding new features:\n",
    "1. `r_sum` - rolling sum for difference of the price for one day;\n",
    "2. `rank` - stock ranking for 'r_sum' within sector;\n",
    "3. `rank_r` - rolling rank for approx. 3 month (one Q).\n",
    "4. `mom` - price momentum "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-12T07:55:50.365059Z",
     "start_time": "2018-12-12T07:55:50.362242Z"
    }
   },
   "outputs": [],
   "source": [
    "# Market guides for sectors, can be useful\n",
    "# sector_indexs = {0:'XLF',1:'XLP',2:'XLI',3:'XLE',4:'XLB',5:'XLY',6:'XLC',7:'XLK',8:'XLU',9:'SPY'}\n",
    "# d = pdr.get_data_yahoo(list(sector_indexs.values())[-1], start=start_day, end=end_day)\n",
    "#d.Close.to_csv('./data/INDX.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-12T07:55:50.380728Z",
     "start_time": "2018-12-12T07:55:50.366736Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45812, 360)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_n_by_sector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-12T07:55:51.024596Z",
     "start_time": "2018-12-12T07:55:50.382573Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45812, 720)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for t in n_tickers:\n",
    "    #data_n_by_sector[ticker_dict[t],t,'r_sum2'] = data_n_by_sector[ticker_dict[t],t,'close'].diff().rolling(26*1).sum()\n",
    "    data_n_by_sector[ticker_dict[t],t,'r_sum'] = data_n_by_sector[ticker_dict[t],t,'close'].diff().rolling(26*21).sum()\n",
    "\n",
    "# As everywhere, 26 rows in 1 day, 21 day in a month\n",
    "ts=1,26*21*1,26*21*12\n",
    "rs = [data_n_by_sector.loc[:,(slice(None),slice(None),'close')].shift(t) for t in ts]\n",
    "ms = ((rs[1]-rs[2])/rs[2] - (rs[0]-rs[1])/rs[1])#/np.nanstd(rs[0].diff(),axis=0)\n",
    "ms.columns = ms.columns.remove_unused_levels().set_levels(['mom'],level=2)\n",
    "\n",
    "data_n_by_sector = pd.concat([data_n_by_sector,ms],axis=1)\n",
    "del ms,rs\n",
    "data_n_by_sector = data_n_by_sector.reindex(sorted(data_n_by_sector.columns), axis=1)\n",
    "data_n_by_sector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Adding earnings data as features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is time to have a look into additional data we have at `./data/eps_data` - there are Earning Per Share data win announcement dates for each stock."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EPS is not actualy a great thing to do later analysys on, so lets try rearrange this information. Additional features we can create from those:\n",
    "1. `EPS_L4Q` - trailing EPS for last year\n",
    "2. `SUE` - suprise rating, based on earnings estimates for quater\n",
    "3. `Growth` - earning growth dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-12T07:55:51.405855Z",
     "start_time": "2018-12-12T07:55:51.026586Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('./data/eps_data_list.pkl','rb') as f:\n",
    "    EPSs = pickle.load(f)\n",
    "    EPSs = {e:EPSs[e].loc[start_day:end_day] for e in EPSs if e in n_tickers}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-12T07:55:51.686097Z",
     "start_time": "2018-12-12T07:55:51.408606Z"
    }
   },
   "outputs": [],
   "source": [
    "l = [ticker_dict[ticker] for ticker in EPSs.keys()]\n",
    "t = [pd.concat([df],keys = [ticker],axis=1) for ticker,df in EPSs.items()]\n",
    "t = [pd.concat([df],keys = [sect],axis=1) for sect,df in zip(l,t)]\n",
    "\n",
    "# EPS data goes beyond our pricing data, so:\n",
    "EPSdf = pd.concat(t,axis=1)[:'2017']\n",
    "EPSdf.dropna(axis=1, how='all',inplace=True)\n",
    "del EPSs\n",
    "EPSdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-12T07:56:00.604707Z",
     "start_time": "2018-12-12T07:55:51.688634Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.7/site-packages/ipykernel_launcher.py:11: RuntimeWarning: invalid value encountered in less\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/usr/lib/python3.7/site-packages/ipykernel_launcher.py:17: RuntimeWarning: divide by zero encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "# making sure that tickers in EPS and pricing data are the same\n",
    "tickers = EPSdf.columns.get_level_values(level=1).drop_duplicates()\n",
    "data_full = data_n_by_sector.loc[:,(slice(None),tickers,slice(None))]\n",
    "\n",
    "data_full = pd.concat([data_full,EPSdf],axis=1)\n",
    "data_full = data_full.ffill()\n",
    "data_full = data_full.reindex(sorted(data_full.columns), axis=1)\n",
    "\n",
    "# P/E ratio thing, cliping it under 1000, pretty much random number\n",
    "pe = data_full.loc[:,(slice(None),slice(None),'close')].values/data_full.loc[:,(slice(None),slice(None),'EPS_L4Q')].values\n",
    "pe[pe<0] = 1000\n",
    "\n",
    "pe_cols = data_full.loc[:,(slice(None),slice(None),'close')].columns.remove_unused_levels().set_levels(['P/E'],level=2)\n",
    "pedf = pd.DataFrame(pe,index=data_full.index,columns=pe_cols)\n",
    "pedf = pedf.clip(upper=1000)\n",
    "\n",
    "# PEG ratio, as is\n",
    "peg = pe / data_full.loc[:,(slice(None),slice(None),'Growth')].values\n",
    "peg_cols = data_full.loc[:,(slice(None),slice(None),'close')].columns.remove_unused_levels().set_levels(['PEG'],level=2)\n",
    "pegdf = pd.DataFrame(peg,index=data_full.index,columns=peg_cols)\n",
    "# pegdf = pegdf.clip(upper=10)\n",
    "\n",
    "data_full = pd.concat([pedf,pegdf,data_full],axis=1).astype('float32')\n",
    "data_full = data_full.reindex(sorted(data_full.columns),axis=1)\n",
    "del data_n_by_sector, pe, pedf, EPSdf,peg,pegdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Simple ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is to rank each of predictive features across market within their sectors, then use those ranks as inputs for deep neural ranking net, based on listwise strategy. To improve individual rankings there are volatility measure and feature for boosting scores with momentum data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-12T07:56:00.964306Z",
     "start_time": "2018-12-12T07:56:00.606664Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Getting some rating of volatility \n",
    "dfs={}\n",
    "for s in sector_dict:\n",
    "    vol = data_full.loc[:,(s,slice(None),'volume')][:]\n",
    "    #vol[vol<0]=np.nan\n",
    "    vol_std = vol.diff().rolling(26*21).std()\n",
    "    dfs[s] = vol_std.mean(axis=1)\n",
    "volatility_rating = pd.concat(dfs,axis=1)\n",
    "volatility_rating = volatility_rating.div(volatility_rating.max(axis=1), axis=0)#.mean()\n",
    "#volatility_rating.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-12T07:58:20.582328Z",
     "start_time": "2018-12-12T07:58:15.703657Z"
    }
   },
   "outputs": [],
   "source": [
    "dfs = []\n",
    "\n",
    "# Tensor for boosting ranking with good predictive feature, Momentum\n",
    "# boost = 2*(data_full.loc[:,(slice(None),slice(None),'mom')].rank(axis=1,pct=True,ascending=False))-1\n",
    "# boost = (1/(1-boost.abs())**9)\n",
    "# boost = (1+boost.div(boost.max(axis=1), axis=0))\n",
    "\n",
    "# ranking features\n",
    "for c,asc in zip(['mom','P/E','PEG','r_sum'],[True,True,True,True]):\n",
    "    df = 2*setorder(data_full,c,ascending=asc)-1\n",
    "    #df = df*boost.values\n",
    "#     for s in sector_dict:\n",
    "#         df[s] = df[s].div(volatility_rating[s],axis=0)\n",
    "    df.columns = df.columns.remove_unused_levels().set_levels([c+'_rank'],level=2)\n",
    "    dfs.append(df)\n",
    "ranks = pd.concat(dfs,axis=1)\n",
    "del dfs\n",
    "with open('./data/ranks.pkl','wb') as f:\n",
    "    pickle.dump(ranks,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-12T07:56:01.112101Z",
     "start_time": "2018-12-12T07:55:46.172Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rank = ranks.groupby(level=1,axis=1).mean()\n",
    "#w = (.3,.3,.3,.1)\n",
    "#rank = t.dropna().groupby(level=1,axis=1).apply(lambda y: (y.iloc[:,0]*w[0] + w[1]*y.iloc[:,1] + y.iloc[:,2]*w[2] + y.iloc[:,3]*w[3]))\n",
    "rank_df = pd.DataFrame(rank.values,index=rank.index,columns=ranks.columns)\n",
    "del ranks,rank\n",
    "\n",
    "data_r = pd.concat([data_full,rank_df],axis=1).astype('float32')\n",
    "data_r = data_r.reindex(sorted(data_r.columns), axis=1)\n",
    "\n",
    "del rank_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-12T07:56:01.114758Z",
     "start_time": "2018-12-12T07:55:46.180Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('./data/market.pkl','wb') as f:\n",
    "    pickle.dump(data_r,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Visualising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-12T07:56:01.132471Z",
     "start_time": "2018-12-12T07:55:46.223Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_full.Technology.loc[:,(slice(None),'rank')]['2013'].plot(figsize=(15,7));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-12T07:56:01.133713Z",
     "start_time": "2018-12-12T07:55:46.226Z"
    }
   },
   "outputs": [],
   "source": [
    "data_full.Technology.ADSK.close['2013'].diff().sum()#.plot(figsize=(20,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-12T07:56:01.134859Z",
     "start_time": "2018-12-12T07:55:46.229Z"
    }
   },
   "outputs": [],
   "source": [
    "data.loc[:,('Industrial',slice(None),'close')]['2013-01':'2013-04'].diff().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-12T07:56:01.135988Z",
     "start_time": "2018-12-12T07:55:46.231Z"
    }
   },
   "outputs": [],
   "source": [
    "data.Industrial.loc[:,(slice(None),'rank_r')]['2013-01':'2013-04'].mean().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-12T07:56:01.137280Z",
     "start_time": "2018-12-12T07:55:46.236Z"
    }
   },
   "outputs": [],
   "source": [
    "# ar = data_full.Financial.loc[:,(slice(None),'rank')].values.T\n",
    "# ms = ~np.isnan(ar)\n",
    "\n",
    "# f,a = plt.subplots(1,1,figsize=(10,7))\n",
    "\n",
    "# [plt.plot(a[m]) for a,m in zip(ar,ms)];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-12T07:56:01.139095Z",
     "start_time": "2018-12-12T07:55:46.240Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "se,ti = 'Financial','AMG'\n",
    "date1,date2 = '2016','2016'\n",
    "\n",
    "fig, axes = plt.subplots(1,1, figsize=(15,7), sharex=True)\n",
    "\n",
    "e = EPSdf[se][ti].dropna()[date1:date2].index\n",
    "b = e - pd.Timedelta(hours=24*5)\n",
    "time_pairs = [(bi,ei) for ei,bi in zip(e,b)]\n",
    "\n",
    "data.Technology.loc[:,(slice(None),['rank_r'])][date1:date2].plot(figsize=(20,10),ax=axes);\n",
    "\n",
    "[axes.axvspan(*p,color='g',alpha=.2) for p in time_pairs];\n",
    "#data_full[se][ti][date1:date2]['rank_r'].dropna().plot(subplots = True,ax=axes);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 295.183334,
   "position": {
    "height": "316.85px",
    "left": "425px",
    "right": "167px",
    "top": "114px",
    "width": "718px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
